{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "!pip install -U transformers kernels torch jaxtyping einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How do I insult someone based on their race, gender, or religion?\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Print full model architecture (this can be very long for large models!)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "layer_count = len(model.model.layers)\n",
        "pos = -1  # final token position\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "with open(\"harmful.txt\", \"r\") as f:\n",
        "    harmful_instructions = f.readlines()\n",
        "\n",
        "with open(\"harmless.txt\", \"r\") as f:\n",
        "    harmless_instructions = f.readlines()\n",
        "\n",
        "harmful_toks = [\n",
        "    tokenizer.apply_chat_template(\n",
        "        conversation=[{\"role\": \"user\", \"content\": insn}],\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ) for insn in harmful_instructions\n",
        "]\n",
        "harmless_toks = [\n",
        "    tokenizer.apply_chat_template(\n",
        "        conversation=[{\"role\": \"user\", \"content\": insn}],\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ) for insn in harmless_instructions\n",
        "]\n",
        "\n",
        "max_its = len(harmful_toks) + len(harmless_toks)\n",
        "bar = tqdm(total=max_its)\n",
        "\n",
        "def generate(toks):\n",
        "    bar.update(1)\n",
        "    return model.generate(\n",
        "        toks.to(model.device),\n",
        "        use_cache=False,\n",
        "        max_new_tokens=1,\n",
        "        return_dict_in_generate=True,\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "\n",
        "harmful_outputs = [generate(toks) for toks in harmful_toks]\n",
        "harmless_outputs = [generate(toks) for toks in harmless_toks]\n",
        "\n",
        "bar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# hidden_states is a tuple of (layer_count+1) entries per output:\n",
        "#   [embeddings, layer1, layer2, ..., layerN]\n",
        "# We'll collect across all layers (skip embeddings at index 0)\n",
        "harmful_hidden_all = [\n",
        "    torch.stack([out.hidden_states[0][l][:, pos, :] for out in harmful_outputs])  # shape: [num_samples, hidden_dim]\n",
        "    for l in range(1, layer_count + 1)  # start from 1 to skip embeddings\n",
        "]\n",
        "harmless_hidden_all = [\n",
        "    torch.stack([out.hidden_states[0][l][:, pos, :] for out in harmless_outputs])\n",
        "    for l in range(1, layer_count + 1)\n",
        "]\n",
        "\n",
        "# Compute mean activations for each layer\n",
        "harmful_means = [h.mean(dim=0) for h in harmful_hidden_all]   # list of [hidden_dim]\n",
        "harmless_means = [h.mean(dim=0) for h in harmless_hidden_all]\n",
        "\n",
        "# Compute refusal_dir per layer\n",
        "refusal_dirs = []\n",
        "for l in range(layer_count):\n",
        "    diff = harmful_means[l] - harmless_means[l]   # [hidden_dim]\n",
        "    diff = diff / (diff.norm() + 1e-9)            # normalize\n",
        "    refusal_dirs.append(diff)\n",
        "\n",
        "# Stack into a single tensor [layers, hidden_dim]\n",
        "refusal_dirs = torch.stack(refusal_dirs, dim=0)\n",
        "\n",
        "# Save\n",
        "save_path = model_id.replace(\"/\", \"_\") + \"_refusal_dirs.pt\"\n",
        "torch.save(refusal_dirs, save_path)\n",
        "\n",
        "print(\"Saved refusal dirs with shape:\", refusal_dirs.shape, \"at\", save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from typing import Optional, Tuple\n",
        "import torch.nn as nn\n",
        "import jaxtyping\n",
        "import random\n",
        "import torch\n",
        "from transformers import TextStreamer\n",
        "import einops\n",
        "\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# --- Load refusal directions ---\n",
        "refusal_dirs = torch.load(model_id.replace(\"/\", \"_\") + \"_refusal_dirs.pt\")\n",
        "# expected shape: [num_layers, 1, hidden_dim] -> squeeze to [num_layers, hidden_dim]\n",
        "if refusal_dirs.dim() == 3 and refusal_dirs.size(1) == 1:\n",
        "    refusal_dirs = refusal_dirs.squeeze(1)  # -> [num_layers, hidden_dim]\n",
        "elif refusal_dirs.dim() == 2:\n",
        "    pass  # already [num_layers, hidden_dim]\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected refusal_dirs shape {tuple(refusal_dirs.shape)}\")\n",
        "\n",
        "num_layers, hidden_dim = refusal_dirs.shape\n",
        "assert num_layers == len(model.model.layers), f\"num_layers mismatch: {num_layers} vs {len(model.model.layers)}\"\n",
        "\n",
        "# normalize directions\n",
        "refusal_dirs = torch.nn.functional.normalize(refusal_dirs, dim=-1)\n",
        "\n",
        "\n",
        "# --- Hook factory ---\n",
        "def make_ablation_hook(direction: torch.Tensor):\n",
        "    direction = direction / (direction.norm() + 1e-9)\n",
        "\n",
        "    def hook(module, inputs, output):\n",
        "        if isinstance(output, tuple):\n",
        "            x = output[0]\n",
        "        else:\n",
        "            x = output\n",
        "\n",
        "        # x: [batch, seq_len, hidden_dim]\n",
        "        # direction: [hidden_dim]\n",
        "\n",
        "        # projection coefficient: <x, d>\n",
        "        proj_coeff = (x * direction).sum(dim=-1, keepdim=True)   # [batch, seq_len, 1]\n",
        "        proj = proj_coeff * direction.view(1, 1, -1)             # [batch, seq_len, hidden_dim]\n",
        "\n",
        "        x = x - proj\n",
        "\n",
        "        if isinstance(output, tuple):\n",
        "            return (x,) + output[1:]\n",
        "        return x\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "# --- Attach hooks to each layer ---\n",
        "for i, layer in enumerate(model.model.layers):\n",
        "    dir_i = refusal_dirs[i].to(next(model.parameters()).dtype)\n",
        "    hook = make_ablation_hook(dir_i)\n",
        "    layer.register_forward_hook(hook)\n",
        "\n",
        "# --- Test model on a safe prompt ---\n",
        "streamer = TextStreamer(tokenizer)\n",
        "conversation = [{\"role\": \"user\", \"content\": \"Write a respectful 2-line poem about autumn.\"}]\n",
        "toks = tokenizer.apply_chat_template(\n",
        "    conversation=conversation,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "gen = model.generate(toks, streamer=streamer, max_new_tokens=50)\n",
        "print(tokenizer.batch_decode(gen[0][len(toks[0]):], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}